{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF8111 - Fouille de données\n",
    "\n",
    "## TP2 Automne 2024 - Exploration de données musicales avec clustering\n",
    "\n",
    "##### Date limite: 09/11\n",
    "\n",
    "##### Membres de l'équipe:\n",
    "\n",
    "    - Nom (Matricule) 1\n",
    "    - Nom (Matricule) 2\n",
    "    - Nom (Matricule) 3\n",
    "\n",
    "##### Livrables :\n",
    "\n",
    "Vous devez soumettre le fichier suivant sur Moodle. Les questions de ce TP totalisent 20 points.\n",
    "1. (Obligatoire) Ce notebook avec votre code.\n",
    "2. (Facultatif) Un rapport PDF comprenant les discussions écrites des questions.\n",
    "\n",
    "##### Présentation\n",
    "\n",
    "Les techniques de clustering peuvent être utilisées pour suivre l'évolution de la musique au fil du temps en regroupant les chansons sur la base de leurs caractéristiques audio, telles que le volume sonore, le tempo, le caractère dansant et l'énergie. En divisant un ensemble de données musicales en périodes spécifiques, des groupes peuvent être formés pour chaque période, révélant ainsi les tendances et les changements dans les styles musicaux. Cette approche permet aux chercheurs de saisir et de visualiser la progression des caractéristiques musicales, en découvrant des modèles qui pourraient ne pas être évidents dans le cadre d'une analyse traditionnelle.\n",
    "\n",
    "Dans ce travail, vous utiliserez des techniques de clustering pour extraire des informations significatives sur la musique au cours des dernières décennies. Enfin, votre objectif est d'utiliser les techniques de regroupement pour construire un système de recommandation pour les utilisateurs qui cherchent des suggestions de nouvelles chansons à écouter. Il est prévu que vous utilisiez les méthodes de regroupement précédentes, mais ne vous y limitez pas. Un degré élevé de créativité dans cette partie sera également récompensé.\n",
    "\n",
    "Toutes les questions seront évaluées sur la base du code écrit, ainsi que de l'explication écrite des résultats (le cas échéant). Lorsqu'elles ne sont pas explicitement interdites, toutes les bibliothèques Python de base (NumPy, Pandas, Scikit-Learn, etc.) peuvent être utilisées. La créativité du code, l'ajout de commentaires (expliquant chaque étape du code) et la vitesse d'exécution du code auront un impact important sur votre évaluation globale. Une question aura un maximum de points si elle s'exécute sous Windows ou Linux, en montrant le résultat attendu et sans lancer d'exceptions. Si la question n'est pas exécutable sous Windows ou Linux, vous perdrez des points. \n",
    "\n",
    "-----\n",
    "\n",
    "## TP2 Autumn 2024 - Music data mining using clustering\n",
    "\n",
    "##### Due date: 09/11\n",
    "\n",
    "##### Team Members:\n",
    "\n",
    "    - Name (Student ID) 1\n",
    "    - Name (Student ID) 2\n",
    "    - Name (Student ID) 3\n",
    "    \n",
    "##### Deliverables:\n",
    "\n",
    "You must submit the following file to Moodle. The questions in this TP total 20 points.\n",
    "1. (Mandatory) This notebook with code.\n",
    "2. (Optional) PDF report including written discussions of the questions.\n",
    "\n",
    "#####  Overview\n",
    "\n",
    "Clustering techniques can be employed to track changes in music over time by grouping songs based on their audio features, such as loudness, tempo, danceability, and energy. By dividing a music dataset into specific time periods, clusters can be formed for each period, revealing trends and shifts in musical styles. This approach allows researchers to capture and visualize the progression of music characteristics, uncovering patterns that might not be evident through traditional analysis.\n",
    "\n",
    "In this work, you will use clustering techniques to extract meaningful information about music over the last decades. Lastly, your goal is to use clustering techniques to build a recommendation system for users looking for suggestions of new songs to hear. You are expected to use the previous clustering methods, but do not feel limited to them. A high degree of creativity in this part will be equally rewarded.\n",
    "\n",
    "All questions will be evaluated based on the written code, as well as the written explanation of the results (when applicable). When not explicitly prohibited, all basic Python libraries (NumPy, Pandas, Scikit-Learn, etc.) can be used. The creativity of the code, the addition of comments (explaining each step of the code), and the speed of code execution will greatly impact your overall evaluation. A question will have maximum points if it runs on Windows or Linux, showing the expected result, and without throwing exceptions. If the question is not executable on Windows or Linux, you will lose its points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 - Construction des méthodes de clustering/Building the Clustering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 - Chargement des données/Loading Data **(0.5 pt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dans ce travail, vous utiliserez l'ensemble de données musicales *TP2_hits*, qui contient diverses caractéristiques des chansons des dernières décennies, telles que les noms des chansons, les noms des artistes et les caractéristiques musicales.\n",
    "\n",
    "##### Pour commencer votre travail, chargez l'ensemble de données musicales *TP2_hits.csv* et affichez ses 5 premiers résultats.\n",
    "\n",
    "-----\n",
    "\n",
    "##### In this work, you will use the *TP2_hits* music dataset, which contains various features of songs from the past decades, such as song names, artist names, and musical characteristics.\n",
    "\n",
    "##### To begin your work, load the *TP2_hits.csv* music dataset and display its top 5 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.226144600Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hits = pd.read_csv('TP2_hits.csv', delimiter=';')\n",
    "hits_copy  = hits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T18:14:15.231016800Z",
     "start_time": "2024-11-04T18:14:15.231016800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hits[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T18:14:15.231016800Z",
     "start_time": "2024-11-04T18:14:15.231016800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explication des données utilisées : \n",
    "- song_name : Le titre de la chanson (par exemple : thank u, next).\n",
    "name_artists : Liste des artistes participant à la chanson, ici présentée sous forme de liste (par exemple : ['Ariana Grande']).\n",
    "- popularity : Score de popularité de la chanson (sur une échelle de 0 à 100). Plus le score est élevé, plus la chanson est populaire.\n",
    "- explicit : Indique si la chanson contient du contenu explicite (valeurs booléennes : True ou False).\n",
    "- song_type : Type de chanson, indiquant si elle est solo ou une collaboration (exemple : Solo).\n",
    "- num_artists : Nombre d'artistes impliqués dans la chanson (par exemple : 1 pour thank u, next).\n",
    "- num_available_markets : Nombre de marchés dans lesquels la chanson est disponible (par exemple : 79).\n",
    "- release_date : Date de sortie de la chanson, formatée en JJ/MM/AAAA.\n",
    "- duration_ms : Durée de la chanson en millisecondes (exemple : 207320 ms correspond à environ 3 minutes et 27 secondes).\n",
    "- key : La clé musicale de la chanson, représentée par un numéro (exemple : 1 correspond à la tonalité Do majeur).\n",
    "- mode : Mode de la chanson (1 = majeur, 0 = mineur).\n",
    "- time_signature : Nombre de temps par mesure, communément 4 pour les chansons populaires.\n",
    "- acousticness : Indice de confiance indiquant si la chanson est acoustique (compris entre 0 et 1).\n",
    "- danceability : Indicateur de la facilité à danser sur la chanson (plus proche de 1 signifie plus dansant).\n",
    "- energy : Mesure de l’énergie et de l’intensité de la chanson (valeur entre 0 et 1).\n",
    "- instrumentalness : Probabilité que la chanson soit instrumentale (valeur proche de 1 = peu de paroles).\n",
    "- liveness : Indique si la chanson a été enregistrée en live (valeur proche de 1 = haute probabilité de live).\n",
    "- loudness : Niveau de volume moyen de la chanson en décibels (dB).\n",
    "- speechiness : Mesure de la présence de paroles parlées (valeurs élevées = plus proche de la parole que du chant).\n",
    "- valence : Indice de positivité musicale (proche de 1 = musique joyeuse et positive).\n",
    "- tempo : Vitesse du morceau en battements par minute (BPM)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 - Transformer et normaliser les données/Transforming and Normalizing Data **(1 pt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lorsque les caractéristiques ont des échelles très différentes, il est important de ramener toutes les valeurs à une échelle commune. Dans cette question, vous devez appliquer les transformations et normalisations nécessaires à l'ensemble de données, en fonction de votre point de vue.\n",
    "\n",
    "##### Conseil: vous devez supprimer les noms des chansons et des artistes et vous concentrer uniquement sur les caractéristiques numériques.\n",
    "\n",
    "-----\n",
    "\n",
    "##### When features have significantly different scales, it is important to bring all the values to a common scale. In this question, you should apply the necessary transformations and normalizations to the dataset, based on your perspective.\n",
    "\n",
    "##### Tip: You should remove the song and artist names and focus only on the numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tout d'abord on transforme les données non-numériques en données normalisées en utilisant Encoder pour Song_type et en créant 2 nouveaux attributs que sont le mois et l'année. En effet nous pensons que ces 2 nouveaux pourraient être pertinent pour notre clustering ( On peut par exemple clusteriser des chansons d'été , ou aussi avoir un cluster pour les chansons des années 80 d'où la pertinence de l'ajout de ces 2 attributs). On normalise par la suite nos données en utilisant Standard_Scaler. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.231016800Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "numerical_data = hits.drop(columns=['song_name', 'name_artists'])\n",
    "numerical_data['explicit'] = numerical_data['explicit'].astype(int)\n",
    "numerical_data['release_date'] = pd.to_datetime(numerical_data['release_date'])\n",
    "#numerical_data['year'] = numerical_data['release_date'].dt.year\n",
    "#numerical_data['month'] = numerical_data['release_date'].dt.month\n",
    "#numerical_data['day'] = numerical_data['release_date'].dt.day\n",
    "numerical_data['song_type'] = encoder.fit_transform(numerical_data['song_type'])\n",
    "numerical_data['release_date_unix'] = numerical_data['release_date'].astype('int64') // 10**9\n",
    "\n",
    "\n",
    "numerical_data.drop(columns='release_date' , inplace=True)\n",
    "hits = pd.DataFrame(scaler.fit_transform(numerical_data) , columns=numerical_data.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "On considère par la suite la corrélation des attributs entre eux."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "corr_matrix = hits.corr()\n",
    "to_drop = set()\n",
    "print(f'Les attributs qui sont fortement corrélées entre eux sont :')\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if np.abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "            print(f'{corr_matrix.columns[i]} ,{corr_matrix.columns[j]}, corrélation {corr_matrix.iloc[i , j]}')\n",
    "            to_drop.add(corr_matrix.columns[i])\n",
    "            \n",
    "hits.drop(columns = to_drop , inplace=True)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.231016800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T18:14:15.246757100Z",
     "start_time": "2024-11-04T18:14:15.246757100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On remarque que num_artists et song_type sont fortement corrélées entre eux. On élimine donc un des 2 attributs (ici num_artists)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 - Clustering avec K-Means/Clustering with K-Means **(3 pt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Une technique de clustering largement utilisée est **K-Means**. K-Means est un algorithme qui répartit les données en un nombre prédéfini de clusters (K). Il assigne chaque point de données au groupe le plus proche en fonction de la distance au centroïde du groupe, qui représente la position moyenne des points au sein de ce groupe.\n",
    "\n",
    "##### Dans cette question, vous devez regrouper l'ensemble de données à l'aide de K-Means et fournir une analyse textuelle des résultats. Votre méthode est-elle efficace pour regrouper les chansons présentant des caractéristiques similaires?\n",
    "\n",
    "##### Les résultats du regroupement par K-Means dépendent fortement des centroïdes initiaux sélectionnés. Que pouvez-vous faire, dans votre code, pour réduire ces effets?\n",
    "\n",
    "##### Enfin, comment sélectionner, dans votre code, le nombre optimal de clusters? Existe-t-il des mesures qui peuvent vous aider?\n",
    "\n",
    "-----\n",
    "\n",
    "##### A widely used clustering technique is **K-Means**. K-Means is an algorithm that partitions data into a predefined number of clusters (K). It works by assigning each data point to the nearest cluster based on the distance to the cluster's centroid, which represents the average position of the points within that cluster.\n",
    "\n",
    "##### In this question, you must cluster the dataset using K-Means and provide a textual analysis of the results. Is your method effective in grouping songs with similar characteristics?\n",
    "\n",
    "##### The results of K-Means clustering are highly dependent on the initial centroids selected. What can you do to reduce these effects?\n",
    "\n",
    "##### Finally, how do you select the optimal number of clusters? Are there any metrics that can help with this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:14:15.252248700Z",
     "start_time": "2024-11-04T18:14:15.252248700Z"
    }
   },
   "outputs": [],
   "source": [
    "### CODE\n",
    "\n",
    "K = range(1,10)\n",
    "inertia_1 = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in K : \n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10).fit(hits)\n",
    "    inertia_1.append(kmeans.inertia_)\n",
    "    if k > 1 : \n",
    "        score = silhouette_score(hits, kmeans.labels_)\n",
    "        silhouette_scores.append(score)\n",
    "\n",
    "# Tracé de l'inertie pour différents k\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(K, inertia_1, 'bo-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "\n",
    "# Tracé du Silhouette Score pour différents k\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(K[1:], silhouette_scores, 'go-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Different k')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.252248700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inertia_4_clusters  = inertia_1[3]\n",
    "#Sauvegarde d'un paramamètre qui nous servira pour la comparaison "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.252248700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "K-means++ choisit les centroîdes d'une manière plus optimale en indexant la probabilité qu'un point soit un centroîde à sa distance avec les autres centroîdes. Autrement dit plus un point est loin des centroîdes plus il aura de d'être choisi comme centroîde. \n",
    "\n",
    "n_init choisit 10 configurations de centroides et détermine laquelle présente le moins d'inertie. \n",
    "On observe une diminution globale de l'inertie globale à mesure que l'on augmente le nombre de clusters. On peut estimer grâce à la méthode du coude le point d'inflexion à 4 clusters.  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 - Réduction de la dimensionnalité et sélection des caractéristiques/Reducing dimension and selecting features **(2.5 pt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lors d'un clustering avec de nombreuses caractéristiques, comme c'est le cas dans ce TP, deux techniques peuvent être utilisées : la **réduction de la dimensionnalité** et/ou la **sélection des caractéristiques**. Ces techniques améliorent les résultats du clustering en réduisant le bruit et en se concentrant sur les données les plus pertinentes, ce qui conduit à des regroupements plus clairs et plus significatifs.\n",
    "\n",
    "##### Dans cette question, vous devez créer une méthode pour réduire les dimensions ou sélectionner les meilleures caractéristiques de l'ensemble de données. Vous êtes libre d'utiliser l'une ou l'autre de ces techniques, ou les deux. Vos résultats seront évalués sur la base de vos métriques, et non sur l'utilisation des deux techniques.\n",
    "\n",
    "##### Après, regroupez les données en utilisant à nouveau K-Means, en employant les mêmes métriques que celles de la Q3. Enfin, rédigez une évaluation textuelle des différences trouvées.\n",
    "\n",
    "-----\n",
    "\n",
    "##### When clustering with many features, as is the case in this TP, two techniques that we can use are **dimensionality reduction** and/or **feature selection**. These techniques enhance clustering results by reducing noise and focusing on the most relevant data, leading to clearer, more meaningful groupings.\n",
    "\n",
    "##### In this question, you must create a method to reduce the dimensions or select the best features from the dataset. It is up to you to use one or both techniques. Your results will be evaluated based on your metrics, not on the use of both techniques.\n",
    "\n",
    "##### Then, cluster the data using K-Means again, employing the same metrics from Q3. Finally, write a textual evaluation of the differences found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.252248700Z"
    }
   },
   "outputs": [],
   "source": [
    "### CODE\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.8)  # retain 90% of variance\n",
    "data_reduced = pca.fit_transform(hits)\n",
    "\n",
    "\n",
    "silhouette_scores_reduced = []\n",
    "    \n",
    "inertia_PCA = []\n",
    "for k in K : \n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10).fit(data_reduced)\n",
    "    inertia_PCA.append(kmeans.inertia_)\n",
    "    \n",
    "    if k > 1:\n",
    "        score = silhouette_score(hits, kmeans.labels_)\n",
    "        silhouette_scores_reduced.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(K, inertia_1, label = \"inertia without PCA\")\n",
    "plt.plot(K, inertia_PCA, 'bo--' , label = \"inertia with PCA\")\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.legend()\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(K[1:], silhouette_scores, 'go-', label='Silhouette scores without PCA')\n",
    "plt.plot(K[1:], silhouette_scores_reduced, 'bo-', label='silouhette scores with PCA')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score after Dimensionality Reduction (PCA)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T18:14:15.261768Z",
     "start_time": "2024-11-04T18:14:15.261768Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ces résultats ne démontrent pas rééllement l'influence de la PCA  car on calcule l'inertie avec les données nouvellement introduites (les nouvelles colonnes) et non l'inertie des données pré-existantes. Pour pouvoir comparer  efficacement le clustering avec la PCA il faudrait calculer les clusters après avoir calculé la PCA et comparer l'inertie avec les données normalisées.\n",
    "\n",
    "\n",
    "Notons que ici le clustering sans PCA avec n_clusters = 4 nous donne une inertie de : 191848 "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inertia = {}\n",
    "n_clusters = 4\n",
    "n_components = range(1 , len(hits.columns)+1 , 2)\n",
    "for k in  n_components: \n",
    "    pca = PCA(n_components= k)  # retain 95% of variance  \n",
    "    data_reduced = pca.fit_transform(hits)\n",
    "    \n",
    "    #print(data_reduced.shape)\n",
    "    kmeans = KMeans(n_clusters= n_clusters, init='k-means++', n_init=10).fit(data_reduced)\n",
    "    hits['cluster'] = kmeans.labels_\n",
    "    hits_dropped = hits.drop('cluster', axis=1)\n",
    "    #print(hits_dropped)\n",
    "    centroids = np.array([hits_dropped[hits['cluster'] == i].mean(axis=0) for i in range(n_clusters)])\n",
    "    iner = sum(np.sum((hits_dropped[hits['cluster'] == i].values - centroids[i])**2) for i in range(n_clusters))\n",
    "    #print(iner.shape)\n",
    "    inertia[k] = iner\n",
    "    \n",
    "hits.drop('cluster', axis=1 , inplace=True)\n",
    "\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.261768Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(n_components , inertia.values(), label = \"inertia with PCA\")\n",
    "plt.xlabel('Number of components for the PCA')\n",
    "plt.ylabel('Inertia calculated with the normalized values')\n",
    "plt.axhline(y=inertia_4_clusters, color='green', linestyle='--', linewidth=2, label='Inertia without PCA ')\n",
    "plt.legend()\n",
    "plt.title('Inertia calculated with PCA')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.261768Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les résultats ne sont pas concluants. En effet, l'inertie totale ne diminue pas moins  que l'inertie d'origine.  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 - Analyse de clustering/Clustering analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 - Évaluer les changements dans la musique avec l'analyse des centroïdes/Evaluating the Changes in Music Using Centroid Analysis **(4 pt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Une autre technique largement utilisée en matière de clustering est l'**analyse centroïde**. L'analyse centroïde est utile pour interpréter les résultats des clusters, car elle révèle les tendances centrales des regroupements et met en évidence les principales caractéristiques et différences entre les groupes.\n",
    "\n",
    "##### Vous allez maintenant travailler en tant que scientifique des données, en utilisant l'analyse centroïde pour examiner l'ensemble de données musicales. Vous devez diviser l'ensemble de données sur les hits en fonction des valeurs *release_date* suivantes:\n",
    "\n",
    "1) De 1995 à 2000  \n",
    "2) De 2001 à 2010  \n",
    "3) De 2011 à 2019  \n",
    "\n",
    "##### Rédigez ensuite une évaluation de l'évolution de la musique sur ces trois tranches temporelles. Vous devriez utiliser l'analyse des centroïdes pour suivre le mouvement des centroïdes des clusters K-Means au fil du temps, ce qui peut indiquer des changements dans les tendances musicales.\n",
    "\n",
    "##### Conseil (non obligatoire): Sélectionnez deux caractéristiques et suivez leurs centroïdes pour observer leur évolution. Toutefois, les approches créatives sont vivement encouragées.\n",
    "\n",
    "-----\n",
    "\n",
    "##### Another widely used technique in clustering is **centroid analysis**. Centroid analysis is useful for interpreting clustering results, as it reveals the central tendencies of clusters and highlights key characteristics and differences between groups.\n",
    "\n",
    "##### Now, you will work as a data scientist, using centroid analysis to examine the music dataset. You must split the hits dataset based on the following *release_date* values:\n",
    "\n",
    "1) From 1995 to 2000  \n",
    "2) From 2001 to 2010  \n",
    "3) From 2011 to 2019  \n",
    "\n",
    "##### Then, write an evaluation of the changes in music across these three time slices. You should use centroid analysis to track the movement of K-Means cluster centroids over time, which can indicate shifts in musical trends.\n",
    "\n",
    "##### Tip (not mandatory): Select two features and track their centroids to observe how they change. However, creative approaches are highly encouraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:24:44.651133200Z",
     "start_time": "2024-11-04T18:24:44.619652100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "hits_copy['release_date'] = pd.to_datetime(hits_copy['release_date'])\n",
    "hits_copy['year'] = hits_copy['release_date'].dt.year\n",
    "\n",
    "old = hits_copy[(hits_copy['year'] >= 1995) & (hits_copy['year'] <= 2000)].select_dtypes(include=['number'])\n",
    "medium =  hits_copy[(hits_copy['year'] >= 2001) & (hits_copy['year'] <= 2010)].select_dtypes(include=['number'])\n",
    "new=  hits_copy[(hits_copy['year'] >= 2011) & (hits_copy['year'] <= 2019)].select_dtypes(include=['number'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "outputs": [],
   "source": [
    "old_centroid = old.mean(axis=0)\n",
    "medium_centroid = medium.mean(axis=0)\n",
    "new_centroid = new.mean(axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T18:24:45.233770300Z",
     "start_time": "2024-11-04T18:24:45.196274900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "outputs": [
    {
     "data": {
      "text/plain": "popularity                   35.562058\nnum_artists                   1.069844\nnum_available_markets        75.144408\nduration_ms              235985.710241\nkey                           5.361963\nmode                          0.694195\ntime_signature                3.944785\nacousticness                  0.248472\ndanceability                  0.623155\nenergy                        0.630348\ninstrumentalness              0.032704\nliveness                      0.190580\nloudness                     -8.454187\nspeechiness                   0.070197\nvalence                       0.606586\ntempo                       117.398495\nyear                       1997.571024\ndtype: float64"
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_centroid"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T18:24:46.383369400Z",
     "start_time": "2024-11-04T18:24:46.367445800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "outputs": [
    {
     "data": {
      "text/plain": "popularity                   36.736013\nnum_artists                   1.100276\nnum_available_markets        73.185382\nduration_ms              226267.851852\nkey                           5.261623\nmode                          0.728526\ntime_signature                3.939716\nacousticness                  0.234339\ndanceability                  0.596593\nenergy                        0.669898\ninstrumentalness              0.036763\nliveness                      0.197661\nloudness                     -7.172347\nspeechiness                   0.077856\nvalence                       0.589318\ntempo                       120.793379\nyear                       2006.158392\ndtype: float64"
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medium_centroid"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T18:24:47.241827400Z",
     "start_time": "2024-11-04T18:24:47.225913400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "outputs": [
    {
     "data": {
      "text/plain": "popularity                   34.314652\nnum_artists                   1.052897\nnum_available_markets        75.188707\nduration_ms              217436.875315\nkey                           5.216835\nmode                          0.713686\ntime_signature                3.946893\nacousticness                  0.270591\ndanceability                  0.604572\nenergy                        0.623664\ninstrumentalness              0.057420\nliveness                      0.196524\nloudness                     -8.067531\nspeechiness                   0.076547\nvalence                       0.561838\ntempo                       121.636294\nyear                       2014.731948\ndtype: float64"
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_centroid"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T18:24:47.560073200Z",
     "start_time": "2024-11-04T18:24:47.544460Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f' old centroid (between 1995 and 2000) :\\n {old_centroid} \\n medium centroids (between 2001 and 2010) : \\n {medium_centroid} \\n new centroids (between 2011 and 2019)  : \\n {new_centroid}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.277402Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = [old_centroid['duration_ms'], medium_centroid['duration_ms'], new_centroid['duration_ms']]  # Duration (in ms) for old, medium, and new centroids\n",
    "Y = [old_centroid['valence'], medium_centroid['valence'], new_centroid['valence']]  # Valence for old, medium, and new centroids\n",
    "\n",
    "# Labels for each point\n",
    "labels = ['1995-2000', '2001-2010', '2011-2019']\n",
    "\n",
    "# Colors for each point\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plot each point with a unique color and label\n",
    "for i, label in enumerate(labels):\n",
    "    plt.scatter(X[i], Y[i], color=colors[i], s=100, label=label)  # s=100 sets the size of the points\n",
    "\n",
    "# Set axis labels and title\n",
    "\n",
    "plt.ylabel('Valence')\n",
    "plt.title('Duration and Valence for Old, Medium, and New Centroids')\n",
    "\n",
    "\n",
    "plt.legend(title='Time Periods')\n",
    "\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.277402Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parmi les deux caractéristiques pertinentes à analyser, on peut sélectionner la durée et la valence. On remarque notamment que la durée des chansons est devenue beaucoup plus courte à mesure que la période a avancé. Ceci est dû à l’accélération du mode de vie général de la population (un mode de vie plus stressant et moins de temps libre). De plus, on observe que la valence (c'est-à-dire le taux de positivité) a également diminué au fil du temps, ce qui pourrait indiquer une tendance plus générale à une population moins joyeuse et plus triste.\n",
    "\n",
    "\n",
    "Parmi les autres caractéristiques intéressantes, on note que le volume sonore a augmenté dans les années 2000 avant de diminuer récemment, probablement en raison de la fin de la \"loudness war\" et de la normalisation du volume sur les plateformes de streaming. L'énergie a suivi un schéma similaire : hausse dans les années 2000 avec des morceaux plus dynamiques, puis une baisse récente en lien avec une préférence croissante pour des musiques plus calmes et introspectives, reflétant une adaptation aux modes de vie plus stressants et à la recherche de détente des auditeurs.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 - Analyse des valeurs aberrantes avec DBSCAN/Analyzing outliers using DBSCAN **(2.5 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) est un algorithme de clustering qui regroupe les points en fonction de leur densité dans l'espace. Il identifie les points centraux, c'est-à-dire ceux qui ont un nombre minimum de points voisins à une certaine distance (*epsilon*). Ces points centraux forment le centre d'un cluster, et tous les points voisins situés à moins de *epsilon* sont affectés à ce cluster.\n",
    "\n",
    "##### Une caractéristique importante de DBSCAN est sa capacité à identifier les valeurs aberrantes potentielles. Cette analyse est cruciale pour identifier les chansons qui diffèrent significativement des autres.\n",
    "\n",
    "##### Dans cette question, vous allez regrouper les données à l'aide de DBSCAN. Tracez les données résultantes à l'aide d'une technique de réduction de la dimensionnalité. Ensuite, sélectionnez trois points aberrants et rédigez une analyse expliquant pourquoi ils sont considérés comme aberrants. La valeur *epsilon* est-elle importante pour identifier ces valeurs aberrantes?\n",
    "\n",
    "-----\n",
    "\n",
    "##### **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that groups points based on their density in space. It works by identifying core points—those that have a minimum number of neighboring points within a certain distance (*epsilon*). These core points form the center of a cluster, and all nearby points within *epsilon* are assigned to that cluster.\n",
    "\n",
    "##### One important characteristic of DBSCAN is its ability to identify potential outliers. This analysis is crucial for identifying songs that differ significantly from others.\n",
    "\n",
    "##### In this question, you will cluster the data using DBSCAN. Plot the resulting data using a dimensionality reduction technique. Then, select three outlier points and write an analysis of why they are considered outliers. Is the *epsilon* value important for identifying these outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_reduced"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.277402Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "pca = PCA(n_components= 2)\n",
    "data_reduced = pca.fit_transform(hits)\n",
    "clusters = dbscan.fit_predict(data_reduced)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Tracé des clusters\n",
    "for i in range(max(clusters) + 1):  # Pour chaque cluster\n",
    "    plt.scatter(data_reduced[clusters == i, 0], data_reduced[clusters == i, 1], color=colors[i], edgecolor='w', s=100, alpha=0.7, label=f'Cluster {i}')\n",
    "\n",
    "# Tracé des outliers (cluster -1)\n",
    "outliers = data_reduced[clusters == -1]\n",
    "plt.scatter(outliers[:, 0], outliers[:, 1], s=100, c='blue', edgecolor='w', label='Outliers' , alpha=0.7)\n",
    "\n",
    "# Sélection de trois points aberrants à entourer\n",
    "selected_outliers = outliers[:3]  # Sélection des trois premiers outliers\n",
    "colors_outliers = ['yellow' , 'pink' , 'purple']\n",
    "\n",
    "\n",
    "for i,point in enumerate(selected_outliers):\n",
    "    plt.scatter(point[0], point[1], s = 250,  edgecolors=colors_outliers[i] , facecolors='none' ,  linewidths=2 , label= f'outlier {i+1}')\n",
    "\n",
    "# Ajouter une légende et afficher le graphique\n",
    "plt.legend(title=\"Clusters\")\n",
    "plt.title('DBSCAN Clustering Visualized with PCA (Outliers Highlighted)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n",
    "\n",
    "# Afficher les trois points sélectionnés\n",
    "print(\"Selected outliers (PCA-transformed):\")\n",
    "for i, point in enumerate(selected_outliers):\n",
    "    print(f\"Outlier {i+1}: {point}\")\n",
    "\n",
    "# Analyser pourquoi ils sont aberrants\n",
    "for i, point in enumerate(selected_outliers):\n",
    "    distances = np.linalg.norm(data_reduced - point, axis=1)\n",
    "    neighbors_within_eps = np.sum(distances < dbscan.eps)\n",
    "    print(f\"Outlier {i+1} has {neighbors_within_eps} neighbors within eps ({dbscan.eps}).\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.277402Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les 3 points sélectionnés ont moins de 5 voisins à une distance epsilon, et tous ces voisins sont soit des border points (des points ayant moins de 5 voisins, mais dont au moins un est un core point), soit des outliers (des points ayant moins de 5 voisins et dont aucun n'est un core point)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 - Analyser les groupes avec le clustering hiérarchique/Analyzing groups with hierarchial clustering **(2.5 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Le clustering hiérarchique** est une méthode de clustering sur la base d'une hiérarchie ou d'une structure arborescente. Elle construit des clusters imbriquées en fusionnant des clusters plus petits (approche agglomérative) ou en divisant des clusters plus grands (approche divisive). Le processus se poursuit jusqu'à ce que tous les points de données se trouvent dans un seul cluster ou que chaque point de données constitue son propre cluster.\n",
    "\n",
    "##### Ce type de regroupement est très utile pour analyser les hiérarchies qui en résultent. Vous pouvez utiliser cette méthode pour examiner les relations entre les clusters et les modèles à plusieurs niveaux qui peuvent ne pas être facilement visibles avec d'autres méthodes.\n",
    "\n",
    "##### Dans cette question, vous allez regrouper les données en utilisant le clustering hiérarchique avec la méthode de Ward. Toutes les caractéristiques sont-elles utiles dans ce regroupement? Représentez les données obtenues dans un dendrogramme. Rédigez ensuite une analyse des résultats obtenus.\n",
    "\n",
    "-----\n",
    "\n",
    "##### **Hierarchical clustering** is a method of grouping data points into clusters based on a hierarchy or tree-like structure. It builds nested clusters by either merging smaller clusters (agglomerative approach) or splitting larger ones (divisive approach). The process continues until all data points are in a single cluster or each data point is its own cluster.\n",
    "\n",
    "##### This type of clustering is very useful for analyzing the resulting hierarchies. You can use this method to examine cluster relationships and multi-level patterns that may not be easily visible with other methods.\n",
    "\n",
    "##### In this question, you will cluster the data using hierarchical clustering with Ward's linkage method. Are all the features useful in this clustering? Plot the resulting data in a dendrogram. Afterward, write an analysis of the results you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.277402Z"
    }
   },
   "outputs": [],
   "source": [
    "### CODE\n",
    "\n",
    "Z = linkage(hits, method='ward')  # Ward's method\n",
    "print(Z)\n",
    "\n",
    "\n",
    "# Création du dendrogramme pour les 10 musiques sélectionnées\n",
    "plt.figure(figsize=(10, 8))\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # Tronquer les autres branches\n",
    "    p=10,  # Afficher seulement les 10 derniers noeuds formés\n",
    "    show_leaf_counts=True\n",
    ")\n",
    "plt.title(\"Hierarchical Clustering Dendrogram - Sample of 10\")\n",
    "plt.xlabel(\"Sample index or (cluster size)\")\n",
    "plt.ylabel(\"Distance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # Tronquer les autres branches\n",
    "    p=10,  # Afficher seulement les 10 derniers noeuds formés\n",
    "    show_leaf_counts=True\n",
    ")\n",
    "plt.title(\"Hierarchical Clustering Dendrogram \")\n",
    "plt.xlabel(\"cluster size\")\n",
    "plt.ylabel(\"Distance\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.277402Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import copy\n",
    "k = 4 \n",
    "cluster_labels = fcluster(Z, k, criterion='maxclust')\n",
    "\n",
    "hits_analysis = copy.copy(hits) \n",
    "hits_analysis['cluster'] = cluster_labels\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.277402Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cluster_means = hits_analysis.groupby('cluster').mean()\n",
    "\n",
    "# Afficher les moyennes de chaque cluster\n",
    "print(cluster_means)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.277402Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "num_columns = len(cluster_means.columns)\n",
    "ncols = 5\n",
    "nrows = (num_columns + ncols - 1) // ncols  # Arrondi à l'entier supérieur pour inclure tous les subplots\n",
    "\n",
    "# Créer la figure et les axes\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 6 * nrows), sharex='col', sharey='row')\n",
    "\n",
    "# Parcourir toutes les variables et créer un graphe pour chaque\n",
    "for i, column in enumerate(cluster_means.columns):\n",
    "    ax = axes[i // ncols, i % ncols]  # Calculer la position correcte de l'axe dans la grille\n",
    "    sns.barplot(x=cluster_means.index, y=cluster_means[column], ax=ax, palette='viridis')\n",
    "    ax.set_title(f'Mean {column} by cluster')\n",
    "    ax.set_xlabel('Cluster Label')\n",
    "    ax.set_ylabel(f'Mean {column}')\n",
    "\n",
    "# Cacher les axes qui ne sont pas utilisés (si le nombre de variables n'est pas un multiple de 5)\n",
    "for j in range(i + 1, nrows * ncols):\n",
    "    axes[j // ncols, j % ncols].set_visible(False)\n",
    "\n",
    "# Ajuster l'espacement entre les subplots et afficher la figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.277402Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.277402Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 - Systèmes de recommandation avec clustering/Recommendation systems using clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8 - Recommander des chansons/Recommending Songs **(4 pt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Les systèmes de recommandation** sont des algorithmes conçus pour suggérer des éléments pertinents aux utilisateurs en fonction de leurs préférences, de leur comportement ou de leurs interactions passées. Ils jouent un rôle crucial dans le filtrage de grandes quantités de données, en fournissant des recommandations personnalisées pour chaque utilisateur.\n",
    "\n",
    "#### Dans cette question, vous devez construire un système de recommandation en utilisant le clustering. En utilisant la technique de votre choix, trouvez les meilleures suggestions musicales pour les chansons suivantes. Cependant, utilisez l'ensemble de données complet, *TP2_nonhits.csv*, pour cette tâche.  Vous pouvez télécharger le jeu de données ici: https://1drv.ms/f/s!AokVPhU6GPPQkv4NipxQg8IFaeRN6w. \n",
    "\n",
    "1) Id 3 - Ariana Grande, *Fake Smile*\n",
    "2) Id 13252 - Kanye West, *All of the Lights* \n",
    "3) Id 284228 - Metallica, *Nothing Else Matters*\n",
    "4) Id 386296 - Céline Dion, *Pour que tu m'aimes encore*  \n",
    "5) Id 511119 - Aerosmith, *Dream On*  \n",
    "\n",
    "#### Comme d'habitude, fournissez une analyse écrite de vos résultats. Toutes les caractéristiques sont-elles utiles? Existe-t-il une technique de clustering plus efficace pour construire un système de recommandation? Est-ce qu'il y a des caractéristiques mieux adaptées pour indiquer différents styles, différents artistes?\n",
    "\n",
    "-----\n",
    "\n",
    "#### **Recommendation systems** are algorithms designed to suggest relevant items to users based on their preferences, behavior, or past interactions. They play a crucial role in filtering vast amounts of data, providing personalized recommendations for each user.\n",
    "\n",
    "#### In this question, you must build a recommendation system using clustering. Using the technique of your choice, find the best music suggestions for the following songs. However, use the full dataset, *TP2_nonhits.csv*, for this task. You can download the dataset here: https://1drv.ms/f/s!AokVPhU6GPPQkv4NipxQg8IFaeRN6w. \n",
    "\n",
    "1) Id 3 - Ariana Grande, *Fake Smile*  \n",
    "2) Id 13252 - Kanye West, *All of the Lights*  \n",
    "3) Id 284228 - Metallica, *Nothing Else Matters*  \n",
    "4) Id 386296 - Celine Dion, *Pour que tu m'aimes encore*  \n",
    "5) Id 511119 - Aerosmith, *Dream On*  \n",
    "\n",
    "#### As usual, provide a written analysis of your results. Are all the features useful? Is there any clustering technique that is more effective for building a recommendation system? Are there features better suited to indicate different styles, different artists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.293786300Z"
    }
   },
   "outputs": [],
   "source": [
    "### CODE\n",
    "non_hits = pd.read_csv('TP2_nonhits.csv', delimiter=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "numerical_data = non_hits.drop(columns=['song_name', 'name_artists'])\n",
    "numerical_data['explicit'] = numerical_data['explicit'].astype(int)\n",
    "numerical_data['release_date'] = pd.to_datetime(numerical_data['release_date'])\n",
    "numerical_data['year'] = numerical_data['release_date'].dt.year\n",
    "numerical_data['month'] = numerical_data['release_date'].dt.month\n",
    "numerical_data['day'] = numerical_data['release_date'].dt.day\n",
    "numerical_data['song_type'] = encoder.fit_transform(numerical_data['song_type'])\n",
    "numerical_data.drop(columns='release_date' , inplace=True)\n",
    "numerical_data.replace('-', np.nan, inplace=True)\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "numerical_data_imputed= imputer.fit_transform(numerical_data)\n",
    "\n",
    "non_hits = pd.DataFrame( scaler.fit_transform(numerical_data_imputed) , columns=numerical_data.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.293786300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numerical_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.293786300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On remarque ici des valeurs manquantes pour certaines valeurs de notre dataset.On remplace donc les données de notre dataset manquantes en utilisant la moyenne (on aurait pu aussi utiliser la médiane). "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rows_with_nan = numerical_data[numerical_data.isna().any(axis=0)]\n",
    "print(rows_with_nan)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.293786300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "non_hits[numerical_data.isna().any(axis=1)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.293786300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-04T18:14:15.293786300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
